package dataDictionary

import org.apache.spark.sql.{DataFrame, SparkSession}

object DataDictionarySQL {
  //val jdbcUsername: String = "root"
  //val jdbcPassword: String = ""

  //to: sqlGlobalDataBasesInfo Topic
  def getGlobalDatabasesInformations(jdbcUrl: String,jdbcUsername: String, jdbcPassword: String )(implicit spark: SparkSession ): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", "  select CATALOG_NAME as catalogName, SCHEMA_NAME as databaseName, DEFAULT_CHARACTER_SET_NAME caracterStyle " +
       // " DEFAULT_COLLATION_NAME as collactionName, SQL_PATH as locationPath "+
        "from information_schema.SCHEMATA " +
        "WHERE schema_name not in ('information_schema','mysql', 'performance_schema','sys', 'phpmyadmin') ")
      .load()
  }

  //to: datadictionarySQLTables Topic
  //ROW_FORMAT, TABLE_ROWS as rowsNumber,
  //TABLE_SCHEMA as databaseName,  TABLE_NAME as tableName, TABLE_TYPE as tableType,  " +
  //        "  DATA_LENGTH as dataLength, AUTO_INCREMENT as autoIncrement, CREATE_TIME as createTime," +
  //        " UPDATE_TIME as updateTime, CHECK_TIME, CREATE_OPTIONS, TABLE_COMMENT
  def getDatabasesTablesDetails_SQL(jdbcUrl: String,jdbcUsername: String, jdbcPassword: String )(implicit spark: SparkSession): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", "  select * from information_schema.tables " +
        " WHERE table_schema not in ('information_schema','mysql', 'performance_schema','sys', 'phpmyadmin') ")
      .load()
  }

  def getDataBaseColumnsDetails(jdbcUrl: String, jdbcUsername: String, jdbcPassword: String )(implicit spark: SparkSession): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " SELECT  TABLE_SCHEMA as databaseName, TABLE_NAME as tableName, COLUMN_NAME as columnName, " +
        "ORDINAL_POSITION as ordinalPosition, COLUMN_DEFAULT as defaultColumn, IS_NULLABLE as nullable, DATA_TYPE as dataType," +
        "CHARACTER_MAXIMUM_LENGTH as maxLength,  COLUMN_TYPE as columnType, COLUMN_KEY as columnKey, " +
        "COLUMN_COMMENT as comment, IS_GENERATED as AutoGenerated FROM information_schema.columns " +
        "WHERE table_schema not in ('information_schema','mysql', 'performance_schema','sys', 'phpmyadmin') LIMIT 20")
      .load()
  }

  def getFilesDetails(jdbcUrl: String,jdbcUsername: String, jdbcPassword: String )(implicit spark: SparkSession): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " select FILE_ID, FILE_NAME, FILE_TYPE, TABLESPACE_NAME, TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, " +
        "LOGFILE_GROUP_NAME, LOGFILE_GROUP_NUMBER, ENGINE, FULLTEXT_KEYS, DELETED_ROWS, UPDATE_COUNT, INITIAL_SIZE, " +
        " MAXIMUM_SIZE, CREATION_TIME, LAST_UPDATE_TIME, LAST_ACCESS_TIME, RECOVER_TIME, TRANSACTION_COUNTER, VERSION, " +
        "ROW_FORMAT, TABLE_ROWS, DATA_LENGTH, MAX_DATA_LENGTH, DATA_FREE, CREATE_TIME, UPDATE_TIME, CHECK_TIME " +
        "  from INFORMATION_SCHEMA.FILES where table_schema not in ('information_schema','mysql', 'performance_schema','sys') ")
      .load()
  }

  def getAllConstraintsDetails(jdbcUrl: String)(implicit spark: SparkSession,jdbcUsername: String, jdbcPassword: String): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " SELECT Distinct information_schema.TABLE_CONSTRAINTS.CONSTRAINT_SCHEMA as constrainDB, information_schema.TABLE_CONSTRAINTS.CONSTRAINT_NAME as constraintName," +
        " information_schema.TABLE_CONSTRAINTS.TABLE_SCHEMA as databaseName, information_schema.TABLE_CONSTRAINTS.TABLE_NAME as tableName, " +
        "information_schema.TABLE_CONSTRAINTS.CONSTRAINT_TYPE as constraintType,  KEY_COLUMN_USAGE.COLUMN_NAME as columnName, KEY_COLUMN_USAGE.ORDINAL_POSITION as ordinalPosition " +
        " FROM information_schema.KEY_COLUMN_USAGE " +
        "LEFT JOIN information_schema.TABLE_CONSTRAINTS  on information_schema.TABLE_CONSTRAINTS.CONSTRAINT_SCHEMA = " +
        "information_schema.KEY_COLUMN_USAGE.CONSTRAINT_SCHEMA left join information_schema.CHECK_CONSTRAINTS" +
        " ON information_schema." +
        "TABLE_CONSTRAINTS.CONSTRAINT_NAME = CHECK_CONSTRAINTS.CONSTRAINT_NAME " +
        " where information_schema.TABLE_CONSTRAINTS.table_schema not in ('information_schema','mysql', 'performance_schema','sys', 'phpmyadmin')")
      .load()
  }

  def getFKConstraintsDetails(jdbcUrl: String)(implicit spark: SparkSession, jdbcUsername: String, jdbcPassword: String  ): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " SELECT DISTINCT CONSTRAINT_CATALOG, CONSTRAINT_SCHEMA, CONSTRAINT_NAME, UNIQUE_CONSTRAINT_CATALOG, " +
        " UNIQUE_CONSTRAINT_SCHEMA, UNIQUE_CONSTRAINT_NAME, MATCH_OPTION, UPDATE_RULE, DELETE_RULE, TABLE_NAME, REFERENCED_TABLE_NAME" +
        " from information_schema.REFERENTIAL_CONSTRAINTS" +
        " where information_schema.TABLE_CONSTRAINTS.table_schema not in ('information_schema','mysql', 'performance_schema','sys', 'phpmyadmin')")
      .load()
  }

  def getKeysDetails(jdbcUrl: String)(implicit spark: SparkSession, jdbcUsername: String, jdbcPassword: String ): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " SELECT CONSTRAINT_CATALOG as constraintCatalog ,CONSTRAINT_SCHEMA as databaseName ,CONSTRAINT_NAME as constraintName" +
        ", TABLE_NAME as tableName, COLUMN_NAME columnName,ORDINAL_POSITION ordinalPosition, POSITION_IN_UNIQUE_CONSTRAINT," +
        " REFERENCED_TABLE_SCHEMA, REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAME FROM information_schema.KEY_COLUMN_USAGE  " +
        "WHERE table_schema not in ('information_schema','mysql', 'performance_schema','sys', 'phpmyadmin')")
      .load();
  }

  def getAllPrivilegesDetails(jdbcUrl: String)(implicit spark: SparkSession, jdbcUsername: String, jdbcPassword: String ): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " select INFORMATION_SCHEMA.USER_PRIVILEGES.GRANTEE as User, INFORMATION_SCHEMA.USER_PRIVILEGES.PRIVILEGE_TYPE " +
        " as privilege , INFORMATION_SCHEMA.USER_PRIVILEGES.IS_GRANTABLE as grantable , INFORMATION_SCHEMA.COLUMN_PRIVILEGES.TABLE_CATALOG " +
        "as TableCatalog,  INFORMATION_SCHEMA.COLUMN_PRIVILEGES.TABLE_SCHEMA , " +
        " INFORMATION_SCHEMA.COLUMN_PRIVILEGES.TABLE_NAME as tableName, INFORMATION_SCHEMA.COLUMN_PRIVILEGES.COLUMN_NAME as ObjectName " +
        " from INFORMATION_SCHEMA.USER_PRIVILEGES left join INFORMATION_SCHEMA.COLUMN_PRIVILEGES " +
        " on INFORMATION_SCHEMA.USER_PRIVILEGES.TABLE_CATALOG = INFORMATION_SCHEMA.COLUMN_PRIVILEGES.TABLE_CATALOG ")
      .load()
  }

  def getUserPrivilegesDetails(jdbcUrl: String)(implicit spark: SparkSession, jdbcUsername: String, jdbcPassword: String ): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " select GRANTEE as user, TABLE_CATALOG tableCatalog, PRIVILEGE_TYPE previlegeType, " +
        "IS_GRANTABLE as grantable from INFORMATION_SCHEMA.USER_PRIVILEGES ")
      .load()
  }

  def getAllTrigersDetails(jdbcUrl: String)(implicit spark: SparkSession, jdbcUsername: String, jdbcPassword: String ): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " select TRIGGER_CATALOG, TRIGGER_SCHEMA, TRIGGER_NAME, EVENT_MANIPULATION, EVENT_OBJECT_CATALOG, " +
        "EVENT_OBJECT_SCHEMA, EVENT_OBJECT_TABLE, ACTION_ORDER, ACTION_CONDITION, ACTION_STATEMENT, ACTION_ORIENTATION, " +
        "ACTION_TIMING, ACTION_REFERENCE_NEW_TABLE, ACTION_REFERENCE_OLD_ROW, ACTION_REFERENCE_NEW_ROW," +
        " CREATED, SQL_MODE, DEFINER, CHARACTER_SET_CLIENT, COLLATION_CONNECTION, DATABASE_COLLATION from INFORMATION_SCHEMA.TRIGGERS limit 5")
      .load()
  }

  //todo: recevoir le now d'une BDD de la search function et répondre par cette méthode
  def getDataBaseTablesDetails(DBName: String, jdbcUrl: String, jdbcUsername: String, jdbcPassword: String )(implicit spark: SparkSession): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", "  select * from information_schema.tables " +
        " WHERE table_schema = '" + DBName + "'and  table_schema not in ('information_schema','mysql', 'performance_schema','sys', 'phpmyadmin')")
      .load()
  }

  //todo: recevoir le now d'une BDD de la search function et répondre par cette méthode
  def getTableColumnsDetails(DBName: String, TableName: String, jdbcUrl: String, jdbcUsername: String, jdbcPassword: String )(implicit spark: SparkSession): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " SELECT * FROM information_schema.columns WHERE table_schema = '" + DBName + "' and table_name ='" + TableName + "'")
      .load();
  }

  def getDataBaseConstraintsDetails(DBName: String, jdbcUrl: String, jdbcUsername: String, jdbcPassword: String )(implicit spark: SparkSession): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " SELECT * FROM information_schema.TABLE_CONSTRAINTS WHERE table_schema = '" + DBName + "'")
      .load();
  }

  def getTableConstraintsDetails(DBName: String, TableName: String, jdbcUrl: String, jdbcUsername: String, jdbcPassword: String )(implicit spark: SparkSession): DataFrame = {
    spark.sqlContext.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("user", jdbcUsername)
      .option("password", jdbcPassword)
      .option("query", " SELECT * FROM information_schema.TABLE_CONSTRAINTS WHERE table_schema = '" + DBName + "' and table_name='" + TableName + "'")
      .load();
  }

}
